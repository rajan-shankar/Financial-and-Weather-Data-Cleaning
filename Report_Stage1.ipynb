{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "## 1.1 Aim\n",
    "We would like to explore an ASX-listed company and how certain external factors influence its share price. In this report, we will step through the data obtaining and cleaning process that will later enable us to perform the relevant analysis.\n",
    "\n",
    "The company we will be looking at is **Costa Group (CGC)**, one of Australia's leading growers of fresh fruit and vegetables. We wish to analyse how **weather conditions** around their farms and **announcements** by their company influence their share price. The time period which we will obtain and prepare data for will be from CGC's ASX listing date (24$^{th}$ July 2015) to the 1$^{st}$ October 2019.\n",
    "\n",
    "## 1.2 Data Collection\n",
    "For our first data set, we need to obtain the historical share prices for CGC. These are conveniently supplied [here](https://au.finance.yahoo.com/quote/CGC.AX/history/) by Yahoo Finance. We can adjust the time period and click 'Download Data' as shown below:\n",
    "\n",
    "<img src=\"https://imgur.com/qu2FNpt.png\" width=\"700\">\n",
    "\n",
    "\n",
    "The downloaded file is of .csv format. It contains a header row with 7 attributes and 1062 other rows, with each row representing the key share price and volume observations for a single day.\n",
    "\n",
    "<br>\n",
    "\n",
    "The second type of data we require is weather data around CGC's farms. Acquiring this data is more complex than our first data set was. Firstly, we need to take a look at CGC's latest financial report to find out where their farms are located. Page 10 of their 2018 six-monthly financial period [report](http://investors.costagroup.com.au/FormBuilder/_Resource/_module/YfnrttzbYEyUJyNrb86SEg/file/report/Six-month_financial_period_2018.pdf), contains this nice visualisation of all of their farms:\n",
    "\n",
    "<img src=\"https://imgur.com/YSVUaLv.png\" width=\"700\">\n",
    "\n",
    "From all of the locations listed above, we are only interested in those that have 'farm' in their name (and also 'Tomato Glasshouse, Guyra). We also do not want to collect weather data for mushroom farms because mushrooms are grown in dark and enclosed areas. We will further simplify our data collection process by excluding the international farms in China and Morocco because the report states that only 1% of CGC's total revenue is generated from international operations (also on page 10).\n",
    "\n",
    "We now need to obtain all of the relevant weather data. The Australian Bureau of Meteorology collects daily weather data from many different weather stations around Australia. We will only collect daily rainfall and maximum temperature data as these two measurements are the main influencers of crop growth at farms.\n",
    "\n",
    "Once at their [website](http://www.bom.gov.au/climate/data/stations/), we need to go through the following steps with all of the farm locations:\n",
    "1. Type the town of the farm into the place name (e.g. Walkamin for “Berry Farm, Walkamin”).\n",
    "2. Select the correct town after clicking “Find place names”.\n",
    "3. Choose “Temperature - maximum” for weather element, and “Daily” for reporting frequency.\n",
    "4. Click “Request or download data”.\n",
    "5. Sort the table by “%”. Try to find a station that is within 50 km with >90% filled records and coverage over the 2015-present time period.\n",
    "6. Download the data and note down the name of that station.\n",
    "7. Now with that same station, obtain its “Rainfall - total” data.\n",
    "8. Move on to the next farm and repeat.\n",
    "\n",
    "For each station and weather measurement type, data is stored in .csv format and contained in a zipped folder. The .csv file contains a header row with 8 attributes and anywhere between 1000 and 50,000 other rows depending on the station, with each row containing information about the station and weather measurement value for a single day. In total, there are 30 .csv files (15 for rain and 15 for temperature, some close-together farms map to the same station).\n",
    "\n",
    "Carrying out all of the above steps should result in the Farm-Station mapping described by the table below:\n",
    "\n",
    "<table style=\"font-size:100%\">\n",
    "    <tr>\n",
    "        <th> Farm </th>\n",
    "        <th> Station </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Gingin </td><td> Gingin Aero </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farms, Tolga <br> Berry Farm, Atherton <br> Banana Farm, Walkamin \n",
    "        <br> Berry Farm, Atherton <br> Avocado Farm, Atherton </td><td> Walkamin Research Station </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Banana Farm, Tully </td><td> Cardwell Marine Pde </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Grape Farm, Mundubbera </td><td> Gayndah Airport </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Avocado Farm, Childers </td><td> Bundaberg Aero </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Corindi </td><td> Coffs Harbour Airport </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Tomato Glasshouse, Guyra </td><td> Guyra Hospital </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Tumbarumba <br> Berry Farm, Rosewood </td><td> Tumbarumba Post Office </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Colignan Citrus Farm </td><td> Mildura Airport </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Yandilla Citrus Farm and Packhouse, Renmark <br> Pike Creek Farm, Lyrup \n",
    "        <br> Amaroo Citrus Farm, Murtho <br> Kangara Citrus Farm and Packhouse, Murtho </td><td> Renmark Aero </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Solora Citrus Farm, Loxton </td><td> Loxton Research Centre </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Sulphur Creek </td><td> Wynyard Airport </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Wesley Vale <br> Berry Farm, East Devonport </td><td> Devonport Airport </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Dunorlan </td><td> Sheffield School Farm </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Berry Farm, Lebrina </td><td> Launceston (Ti Tree Bend) </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "The final type of data we require are CGC's official company announcements. These are available as navigable tables on many different websites, but the one on Commonwealth Securities' [website](https://www2.commsec.com.au/quotes/?stockCode=CGC#/) (n.b. you need to log in with a CommSec trading account) can be customised to display an exhaustive list of CGC's announcements on one scrollable page. This allows us to carry out a quick web-scrape of the table (in Google Chrome) like so:\n",
    "1. Press F12 to bring up the HTML DevTools panel.\n",
    "2. Click the element inspector tool on the top-left of the panel.\n",
    "3. Capture the table as shown in the picture below.\n",
    "4. Copy the `<tbody data-v-0ea6aa3f>...</tbody>` section and paste it into a .txt file.\n",
    "5. Delete `<tbody data-v-0ea6aa3f=\"\">` from the first line of the .txt file and delete `</tbody>` from the end of the last line of the .txt file.\n",
    "\n",
    "<img src=\"https://imgur.com/VgdkOlw.png\" width=\"1000\">\n",
    "\n",
    "Unlike the other data files we obtained, our CGC announcements file is a .txt file of HTML code. The file contains no header row (we will add this in during our data cleaning) but has 583 other rows. Each row consists of 5 attributes detailing the title of the announcement document along with time, date, number of pages and whether or not the announcement is market sensitive (this will be an important attribute for later analysis). The attribute values in each row are divided by `<td></td>` tags, and we will use this feature to our advantage later in our data cleaning.\n",
    "\n",
    "## 1.3 Data Usage Rights\n",
    "The share price data and announcements data are both sourced from the ASX even though they are displayed by Yahoo Finance and CommSec on their websites. The ASX [Terms of Use](https://www.asx.com.au/about/terms-use.htm) states that \n",
    "> *You must not use the Content for commercial purposes without first obtaining the express written authority of ASX. Use of the Content for a commercial purpose is any use other than accessing and using the content for your own personal and private decision making.*\n",
    "\n",
    "... where \"Content\" is defined as\n",
    "> *all information, text, materials, graphics, software, tools, results derived from the use of software and tools, advertisements, names, logos and trade marks on the Site*\n",
    "\n",
    "Since our data is ultimately sourced with on-site tools and information tables, and we are strictly only using the data for personal reasons, we are allowed to use the data we have sourced.\n",
    "\n",
    "As for the weather data, the Bureau of Meteorology's data is protected by the Copyright Act 1968. We are allowed to use the data non-commercially as long as we have referenced where we sourced the data from (see Section 4 of this report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Cleaning and Transformation\n",
    "## 2.1 Preliminary Code\n",
    "Before we begin our file cleaning and transformation process, we need to import any modules and define any functions that we will be using during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # read and write to .csv files\n",
    "import glob # get names of files in a directory\n",
    "import datetime as dt # work with date attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_bad_values` function below will be used on each attribute of each resultant file after the cleaning process to check if there remain any missing or out-of-range values. Given a file name, column number and an expected range (only for numeric attributes), the function will print out any rows containing bad values under the specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_values(file_name, column_no, minimum='no minimum', maximum='no maximum'):\n",
    "    data = list(csv.reader(open(file_name)))\n",
    "    for row in data[1:]: # assume there's a header row\n",
    "        if row[column_no] != '':\n",
    "            if minimum != 'no minimum' and maximum != 'no maximum':\n",
    "                try:\n",
    "                    if float(row[column_no]) < minimum or float(row[column_no]) > maximum:\n",
    "                        print(row) # out-of-range value found\n",
    "                except:\n",
    "                    print(row) # float conversion failure\n",
    "        else:\n",
    "            print(row) # missing value found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view_data` function below allows data that is in list-form to be displayed concisely. It takes the name of the list as input and prints out the first four and last three elements of the list. Usually, the first element of the list will be its header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_data(list_name):\n",
    "    for row in list_name[:4]: print(row)\n",
    "    print('  ⋮')\n",
    "    for row in list_name[-3:]: print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Share Price Data \n",
    "With the help of our imported `csv` module, we will read in our .csv file of share prices and store it as a list in `share_price_data_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
      "['2015-07-24', '2.240000', '2.270000', '2.160000', '2.160000', '1.972063', '23179807']\n",
      "['2015-07-27', '2.190000', '2.190000', '2.160000', '2.160000', '1.972063', '2417254']\n",
      "['2015-07-28', '2.150000', '2.160000', '2.010000', '2.040000', '1.862504', '2379886']\n",
      "  ⋮\n",
      "['2019-09-27', '3.600000', '3.645000', '3.530000', '3.530000', '3.530000', '2260278']\n",
      "['2019-09-30', '3.530000', '3.795000', '3.510000', '3.750000', '3.750000', '2826881']\n",
      "['2019-10-01', '3.750000', '3.770000', '3.650000', '3.730000', '3.730000', '1910395']\n"
     ]
    }
   ],
   "source": [
    "share_price_data_raw = list(csv.reader(open('CGC.AX.csv')))\n",
    "view_data(share_price_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fairly clean as it is. Notice that the rows are listed chronologically, starting from CGC's ASX listing date of 2015-07-24 (already in the ISO 8601 standards format). Dates such as 2015-07-25 are missing because they are either weekends or national holidays; the ASX share market does not operate during these days. We do not need to do anything about that.\n",
    "\n",
    "However, we would like to reformat the price-associated attributes so that they better represent dollar amounts, i.e. to two decimal places and right-padded with zeroes where required. We will define a function that carries out this conversion below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monetary_display(value):\n",
    "    price = str(round(float(value), 2))\n",
    "    if len(price.split('.')[1]) == 1:\n",
    "        price += '0'\n",
    "    return str(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to remove the `Adj Close` attribute from the data because it is just a more complex version of the already-existing `Close` attribute and we would like to keep things simple for future analysis. Hence, we will develop a cleaned data set and store it in the list `share_price_data_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "['2015-07-24', '2.24', '2.27', '2.16', '2.16', '23179807']\n",
      "['2015-07-27', '2.19', '2.19', '2.16', '2.16', '2417254']\n",
      "['2015-07-28', '2.15', '2.16', '2.01', '2.04', '2379886']\n",
      "  ⋮\n",
      "['2019-09-27', '3.60', '3.65', '3.53', '3.53', '2260278']\n",
      "['2019-09-30', '3.53', '3.79', '3.51', '3.75', '2826881']\n",
      "['2019-10-01', '3.75', '3.77', '3.65', '3.73', '1910395']\n"
     ]
    }
   ],
   "source": [
    "share_price_data_final = [['Date', 'Open', 'High', 'Low', 'Close', 'Volume']] # header row\n",
    "\n",
    "for row in share_price_data_raw[1:]:\n",
    "    date = row[0]\n",
    "    open_price = monetary_display(row[1])\n",
    "    high_price = monetary_display(row[2])\n",
    "    low_price = monetary_display(row[3])\n",
    "    close_price = monetary_display(row[4])\n",
    "    volume = row[6]\n",
    "    share_price_data_final.append([date, open_price, high_price, low_price, close_price, volume])\n",
    "    \n",
    "view_data(share_price_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `share_price_data_final` list can now be written to a .csv file. Let's also view the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "['2015-07-24', '2.24', '2.27', '2.16', '2.16', '23179807']\n",
      "['2015-07-27', '2.19', '2.19', '2.16', '2.16', '2417254']\n",
      "['2015-07-28', '2.15', '2.16', '2.01', '2.04', '2379886']\n",
      "  ⋮\n",
      "['2019-09-27', '3.60', '3.65', '3.53', '3.53', '2260278']\n",
      "['2019-09-30', '3.53', '3.79', '3.51', '3.75', '2826881']\n",
      "['2019-10-01', '3.75', '3.77', '3.65', '3.73', '1910395']\n"
     ]
    }
   ],
   "source": [
    "with open('Share_Price_Data.csv', 'w', newline='\\n') as myfile:\n",
    "    for entry in share_price_data_final:\n",
    "        csv.writer(myfile).writerow(entry)\n",
    "        \n",
    "view_data(share_price_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform a scan of all of the attributes of the new file to ensure that there are no bad values using our `find_bad_values` function. If there is nothing printed out after the code, then we are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_bad_values('Share_Price_Data.csv', 0) # Date\n",
    "for column in range(1, 5): find_bad_values('Share_Price_Data.csv', column, 0, 50) # Open, High, Low, Close\n",
    "find_bad_values('Share_Price_Data.csv', 5, 0, 10e7) # Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Weather Data\n",
    "The cleaning and transformation process for our weather data will be a lot more involved because we have to combine 30 different .csv files into one.\n",
    "\n",
    "We will begin by extracting all of the dates in our `share_price_data_final` list from before into another list called `asx_dates`. When we go to process our weather data files, we can check against the `asx_dates` list to make sure we only keep weather observations between the 24$^{th}$ July 2015 to the 1$^{st}$ October 2019 on days when the ASX share market operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-24\n",
      "2015-07-27\n",
      "2015-07-28\n",
      "2015-07-29\n",
      "  ⋮\n",
      "2019-09-27\n",
      "2019-09-30\n",
      "2019-10-01\n"
     ]
    }
   ],
   "source": [
    "asx_dates=[]\n",
    "for row in share_price_data_final[1:]:\n",
    "    asx_dates.append(row[0])\n",
    "\n",
    "view_data(asx_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, let's look at one of the weather data files to get an idea of the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product code', 'Bureau of Meteorology station number', 'Year', 'Month', 'Day', 'Rainfall amount (millimetres)', 'Period over which rainfall was measured (days)', 'Quality']\n",
      "['IDCJAC0009', '024024', '1984', '01', '01', '', '', '']\n",
      "['IDCJAC0009', '024024', '1984', '01', '02', '', '', '']\n",
      "['IDCJAC0009', '024024', '1984', '01', '03', '', '', '']\n",
      "  ⋮\n",
      "['IDCJAC0009', '024024', '2019', '09', '30', '0.0', '1', 'N']\n",
      "['IDCJAC0009', '024024', '2019', '10', '01', '0.0', '1', 'N']\n",
      "['IDCJAC0009', '024024', '2019', '10', '02', '0.0', '1', 'N']\n"
     ]
    }
   ],
   "source": [
    "loxton_rainfall = list(csv.reader(open('Weather Files/Loxton_rain.csv')))\n",
    "view_data(loxton_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we would like to do regarding the attributes of the data set. We want to replace the `Bureau of Meteorology station number` attribute with the name of the station it represents and the state that the station is located in. We can create a dictionary relating each station number (for all the stations that we have .csv files) to its name and state like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary with key=stationnumber, value=['stationname','state']\n",
    "number_name = {\n",
    "    9178 : [\"Gingin Aero\",\"WA\"],\n",
    "    31108 : [\"Walkamin Research Station\",\"QLD\"],\n",
    "    32004 : [\"Cardwell Marine Pde\",\"QLD\"],\n",
    "    39066 : [\"Gayndah Airport\",\"QLD\"],\n",
    "    39128 : [\"Bundaberg Aero\",\"QLD\"],\n",
    "    59151 : [\"Coffs Harbour Airport\",\"NSW\"],\n",
    "    56229 : [\"Guyra Hospital\",\"NSW\"],\n",
    "    72043 : [\"Tumbarumba Post Office\",\"NSW\"],\n",
    "    91107 : [\"Wynyard Airport\",\"TAS\"],\n",
    "    91126 : [\"Devonport Airport\",\"TAS\"],\n",
    "    91291 : [\"Sheffield School Farm\",\"TAS\"],\n",
    "    76031 : [\"Mildura Airport\",\"VIC\"],\n",
    "    24048 : [\"Renmark Aero\",\"SA\"],\n",
    "    24024 : [\"Loxton Research Centre\",\"SA\"],\n",
    "    91237 : [\"Launceston (Ti Tree Bend)\", \"TAS\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to merge and delete some attributes. The `Product code`, `Period over which rainfall was measured (days)` and `Quality` are not very useful to us, so we will remove them. The `Year`, `Month` and `Day` should be merged into one attribute that showcases the date in ISO 8601 standard format.\n",
    "\n",
    "We will do a similar thing with the temperature files, and also remove the `Days of accumulation of maximum temperature` attribute in those files.\n",
    "\n",
    "Upon observing the rows in the rainfall and temperature files, we notice that some of them are missing values for the `Total Rainfall (mm)` and `Max Temperature (degrees Celsius)` attributes. We would like to fill in these missing values with the average of the most recent day that had a measurement and the next closest day in the future that has a measurement. For example:\n",
    "\n",
    "<img src=\"https://imgur.com/EpbYlqj.png\" width=\"800\">\n",
    "\n",
    "Finally, we would like to join the `Max Temperature (degrees Celsius)` attribute of each station's temperature file onto its corresponding station's rainfall, and then combine all of the 15 resultant files into one big final .csv file.\n",
    "\n",
    "All of the procedures we wish to do mentioned above are carried out in the following chunk of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_final = [[\"Station Name\", \"State\", \"Date\", \"Total Rainfall (mm)\", \"Max Temperature (degrees Celsius)\"]] # header\n",
    "directory = list(glob.iglob('Weather Files/*.csv')) # get all .csv files in the Weather Files directory\n",
    "directory.sort()\n",
    "\n",
    "for file in directory[::2]:\n",
    "    \n",
    "    # at each go, input the rainfall csv file max_temp csv file for the same station\n",
    "    # e.g. Walkamin_rain.csv Walkamin_temp.csv\n",
    "    rainfile=file\n",
    "    tempfile=directory[directory.index(file) + 1]\n",
    "\n",
    "    #two for-loops for rain csv file. two for-loops for temp file\n",
    "    no_rain_value={}\n",
    "    no_temp_value={}\n",
    "    rainfall_lines=[]\n",
    "\n",
    "    f=list(csv.reader(open(rainfile)))\n",
    "    g=list(csv.reader(open(tempfile)))\n",
    "\n",
    "    #first for-loop finds missing entries in rainfall file and alocates them values using the mean of non-empty adjacent entries\n",
    "    i=1\n",
    "    while i<len(f):\n",
    "        line_list=f[i]\n",
    "        rainfall=line_list[5]\n",
    "        if rainfall==\"\":# locate an empty value\n",
    "            #locate the value before which is not empty\n",
    "            # since integers are immutable, if we assign i to something, then change that someting, i is not changed\n",
    "            year=line_list[2]\n",
    "            month=line_list[3]\n",
    "            day=line_list[4]\n",
    "            ISOdate=year+\"-\"+month.zfill(2)+\"-\"+day.zfill(2)\n",
    "\n",
    "            left=None\n",
    "            right=None\n",
    "            j=i-1\n",
    "            #loop backwards to get a value for the top side to average\n",
    "            while j>=1:\n",
    "                if f[j][5]!='':\n",
    "                    left=float(f[j][5])\n",
    "                    break\n",
    "                j-=1\n",
    "            #this method only works if the very first data value is filled.\n",
    "\n",
    "            #loop forward to get a value for the bottom value to average\n",
    "            k=i+1\n",
    "            while k<len(f):# only works if the last data value is filled\n",
    "                if f[k][5]!='':\n",
    "                    right=float(f[k][5])\n",
    "                    break\n",
    "                k+=1\n",
    "            if left!=None and right!=None:\n",
    "                # if either the left value or the right value cannot be initialised, we dont add the date to the dictionary.\n",
    "                #the result is, that the missing day will have an empty string in the final dataset.\n",
    "                average=round((left+right)/2, 1)\n",
    "\n",
    "                # each date is unique so there will be no such thing as repeats in the dictionary\n",
    "                no_rain_value[ISOdate]=average\n",
    "        i+=1\n",
    "\n",
    "    # second for-loop. this appends strings to rainfall_lines. each string is a line to be appended to the final file.\n",
    "    # each string is in the form:'station_name,state,ISOdate,daily rainfall (mm)'\n",
    "    is_first_line=True\n",
    "    station_name=None\n",
    "    for line in f:\n",
    "        if is_first_line:\n",
    "            is_first_line=False\n",
    "        else:\n",
    "            line_list = line\n",
    "            for number in number_name:\n",
    "                if number==int(line_list[1]):\n",
    "                    station_name=number_name[number][0]\n",
    "                    state=number_name[number][1]\n",
    "            assert station_name!=None\n",
    "\n",
    "            rainfall=line_list[5]\n",
    "            ISOdate=line_list[2]+\"-\"+line_list[3].zfill(2)+\"-\"+line_list[4].zfill(2)\n",
    "            ISOdate=ISOdate.strip()\n",
    "\n",
    "            if ISOdate in asx_dates:\n",
    "                if ISOdate in no_rain_value:\n",
    "                    line_to_append=station_name+','+state+','+ISOdate+','+str(no_rain_value[ISOdate])\n",
    "                    rainfall_lines.append(line_to_append)\n",
    "                else:\n",
    "                    line_to_append=station_name+','+state+','+ISOdate+','+str(rainfall)\n",
    "                    rainfall_lines.append(line_to_append)\n",
    "\n",
    "    # third for-loop. loop through the max_temp file for missing entries. allocates them values using the mean of non-empty\n",
    "    # adjacent entries\n",
    "    i=1\n",
    "    while i<len(g):\n",
    "        line_list=g[i]\n",
    "        temp=line_list[5]\n",
    "        if temp=='': # locate a missing value\n",
    "\n",
    "            year=line_list[2]\n",
    "            month=line_list[3]\n",
    "            day=line_list[4]\n",
    "            ISOdate=year+\"-\"+month.zfill(2)+\"-\"+day.zfill(2)\n",
    "\n",
    "            left=None\n",
    "            right=None\n",
    "            j=i-1\n",
    "            #loop backwards to get a value for the top side to average\n",
    "            while j>=1:\n",
    "                if g[j][5]!='':\n",
    "                    left=float(g[j][5])\n",
    "                    break\n",
    "                j-=1\n",
    "            #this method only works if the very first data value is filled.\n",
    "\n",
    "            #loop forward to get a value for the bottom value to average\n",
    "            k=i+1\n",
    "            while k<len(g):# only works if the last data value is filled\n",
    "                if g[k][5]!='':\n",
    "                    right=float(g[k][5])\n",
    "                    break\n",
    "                k+=1\n",
    "\n",
    "            if left!=None and right!=None:\n",
    "                # if either the left value or the right value cannot be initialised, we dont add the date to the dictionary.\n",
    "                #the result is, that the missing day will have an empty string in the final dataset.\n",
    "                average=round((left+right)/2, 1)\n",
    "                no_temp_value[ISOdate]=average\n",
    "        i+=1\n",
    "\n",
    "    # fourth for-loop. this one appends to weather_data_final\n",
    "    is_first_line=True\n",
    "    for line in g:\n",
    "        if is_first_line:\n",
    "            is_first_line=False\n",
    "        else:\n",
    "            line_list=line\n",
    "            ISOdate=line_list[2]+\"-\"+line_list[3].zfill(2)+\"-\"+line_list[4].zfill(2)\n",
    "            ISOdate=ISOdate.strip()\n",
    "\n",
    "            if ISOdate in asx_dates:\n",
    "                if ISOdate in no_temp_value:\n",
    "                    #search for the corresponding rainfall line\n",
    "                    temp=no_temp_value[ISOdate]\n",
    "                    for string in rainfall_lines:\n",
    "                        if ISOdate in string:\n",
    "                            line=string+\",\"+str(temp)\n",
    "                            weather_data_final.append(line.split(\",\"))\n",
    "                else:\n",
    "                    temp=float(line_list[5])\n",
    "                    for string in rainfall_lines:\n",
    "                        if ISOdate in string:\n",
    "                            line=string+\",\"+str(temp)\n",
    "                            weather_data_final.append(line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `weather_data_final` list holds the final data that we can now write to a .csv file. Let's also take a glimpse at the `weather_data_final` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station Name', 'State', 'Date', 'Total Rainfall (mm)', 'Max Temperature (degrees Celsius)']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-24', '0.2', '26.5']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-27', '0.0', '26.4']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-28', '0.0', '23.2']\n",
      "  ⋮\n",
      "['Wynyard Airport', 'TAS', '2019-09-27', '0.0', '14.4']\n",
      "['Wynyard Airport', 'TAS', '2019-09-30', '0.0', '17.1']\n",
      "['Wynyard Airport', 'TAS', '2019-10-01', '0.0', '19.1']\n"
     ]
    }
   ],
   "source": [
    "with open('Weather_Data.csv', 'w', newline='\\n') as myfile:\n",
    "    for entry in weather_data_final:\n",
    "        csv.writer(myfile).writerow(entry)\n",
    "                   \n",
    "view_data(weather_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform a scan of all of the attributes of our new Weather_Data.csv file to ensure that there are no missing or out-of-range values using our `find_bad_values` function. If there is nothing printed out after the code, then we are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in range(0,3): find_bad_values('Weather_Data.csv', column) # Station Name, State, Date\n",
    "find_bad_values('Weather_Data.csv', 3, 0, 1000) # Rainfall\n",
    "find_bad_values('Weather_Data.csv', 4, -20, 60) # Max Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Announcements Data\n",
    "The way we will go about cleaning our announcements data will be different yet again. Firstly, we will see how the data looks in the .txt file we obtained earlier:\n",
    "\n",
    "<img src=\"https://imgur.com/rU8PIjv.png\" width=\"800\">\n",
    "\n",
    "The rows are not actually broken up properly and there are many tab characters amongst the data that we do not want. As a result, we will instead remove all tab `\\t` and newline `\\n` characters from the data (essentially making it one big block of text) and split it back into rows on the `</tr>` tags (this is how table rows are naturally defined in HTML). We will carry out this procedure and read the data into a list simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr data-v-f1feab4e=\"\"><td data-v-f1feab4e=\"\">17/09/2019</td> <td data-v-f1feab4e=\"\">4:43 pm AEST</td> <td data-v-f1feab4e=\"\"><a data-v-f1feab4e=\"\" href=\"https://commsec.api.markitondemand.com/commsec-node-api/1.0/event/document/1410-02147946-56P2ILQ5KQ6N7DVPVGH8U1U219/pdf?access_token=1e1dH4OusefhjY5YejYoO4C5EoDT\" target=\"_blank\">Change in substantial holding</a></td> <td data-v-f1feab4e=\"\" class=\"text-center\">7</td> <td data-v-f1feab4e=\"\" class=\"text-center\"><!----></td>\n"
     ]
    }
   ],
   "source": [
    "announcements_data_raw = open('Announcements_CommSec.txt').read().replace('\\t', '').replace('\\n', '').split('</tr>')[:-1]\n",
    "print(announcements_data_raw[0]) # we are not using our view_data() function because otherwise the output becomes very messy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely, we can see that the bits of data we are interested in are surrounded by `<td...></td>` tags (these define cells in a row in HTML). We will exploit this characteristic to extract the data we require. However, even once we have done that, there is still some cleaning to do. We will need to convert the date (17/09/2019) into the ISO 8601 standard format (2019-09-17), and we will need to split the time attribute (4:43 pm AEST) into a zero-padded 24-hr time attribute (16:43) and a timezone attribute (AEST). We will carry this whole process out and store the result in the `announcements_data_final` list below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "announcements_data_final = []\n",
    "for row in announcements_data_raw:\n",
    "    fields = row.split('<td data-v-f1feab4e=\"\"')   \n",
    "    date = dt.datetime.strptime(fields[1], '>%d/%m/%Y</td> ').strftime('%Y-%m-%d') # pull date apart and glue back in ISO format\n",
    "    \n",
    "    # time processing\n",
    "    time_raw = fields[2].replace('</td> ', '').replace('>', '')\n",
    "    hour = time_raw[0:-11]\n",
    "    minute = time_raw[-10:-8]\n",
    "    meridiem = time_raw[-7:-5]    \n",
    "    if meridiem == 'pm':\n",
    "        hour = str(int(hour) + 12)\n",
    "        if hour == '24':\n",
    "            hour = '12'\n",
    "    elif meridiem == 'am':\n",
    "        if hour == '12':\n",
    "            hour == '00'\n",
    "    hour = hour.zfill(2)\n",
    "    \n",
    "    time_cleaned = hour + ':' + minute\n",
    "    time_zone = time_raw[-4:]\n",
    "    title = fields[3].split('target=\"_blank\">')[1].replace('</a></td> ', '').replace('&amp;', '&')\n",
    "    pages = fields[4].split('>')[1].replace('</td', '')\n",
    "    market_sensitive = 'No' if fields[5].split('>')[1] == '<!----' else 'Yes'\n",
    "\n",
    "    announcements_data_final.append([date, time_cleaned, time_zone, title, pages, market_sensitive])\n",
    "    \n",
    "announcements_data_final.append(['Date', '24-hr Time', 'Timezone', 'Title', 'Pages', 'Market Sensitive']) # header row\n",
    "announcements_data_final.reverse()  # chronological ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done before, we will write the `announcements_data_final` list to a .csv file and view the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', '24-hr Time', 'Timezone', 'Title', 'Pages', 'Market Sensitive']\n",
      "['2015-07-24', '09:54', 'AEST', 'ASX Market Release - Admission and Official Quotation', '1', 'Yes']\n",
      "['2015-07-24', '09:55', 'AEST', 'ASX Market Release - Pre-quotation disclosure', '1', 'No']\n",
      "['2015-07-24', '09:58', 'AEST', 'Appendix 1A and Information Form and Checklist', '20', 'No']\n",
      "  ⋮\n",
      "['2019-09-05', '11:38', 'AEST', 'Becoming a substantial holder', '4', 'No']\n",
      "['2019-09-09', '16:40', 'AEST', 'Becoming a substantial holder', '7', 'No']\n",
      "['2019-09-17', '16:43', 'AEST', 'Change in substantial holding', '7', 'No']\n"
     ]
    }
   ],
   "source": [
    "with open('Announcements_Data.csv', 'w', newline='\\n') as myfile:\n",
    "    for entry in announcements_data_final:\n",
    "        csv.writer(myfile).writerow(entry)\n",
    "                   \n",
    "view_data(announcements_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform a scan of all of the attributes of our new Announcements_Data.csv file to ensure that there are no missing or out-of-range values using our `find_bad_values` function. If there is nothing printed out after the code, then we are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in range(0, 4): find_bad_values('Announcements_Data.csv', column) # Date, 24-hr Time, Timezone, Title\n",
    "find_bad_values('Announcements_Data.csv', 4, 1, 500) # Pages\n",
    "find_bad_values('Announcements_Data.csv', 5) # Market Sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Merging Datasets\n",
    "In this section, we will attempt to merge our three freshly-cleaned sets of data. The key attribute that is common amongst all three data sets is the `Date` attribute. The share prices and weather data both contain entries for all ASX operating dates between 2015-07-24 and 2019-10-01, so we will simply attach each share price row to wherever there is a date match.\n",
    "\n",
    "Adding on the announcements data is a little more difficult. Announcements are not released every single day, and sometimes there are more than one released on a single day. Hence, for each date match with a weather/share price data entry, we will attach all announcements that were released on that day, divided by '|' characters within each announcement attribute. This is a bit confusing, but it will make more sense once we view the final merged data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station Name', 'State', 'Date', 'Total Rainfall (mm)', 'Max Temperature (degrees Celsius)', 'Open', 'High', 'Low', 'Close', 'Volume', '24-hr Time', 'Timezone', 'Title', 'Pages', 'Market Sensitive']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-24', '0.2', '26.5', '2.24', '2.27', '2.16', '2.16', '23179807', '09:54|09:55|09:58|09:58|09:59|10:02|10:04|10:04|10:06|10:07|10:07|10:09|10:09|', 'AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|AEST|', 'ASX Market Release - Admission and Official Quotation|ASX Market Release - Pre-quotation disclosure|Appendix 1A and Information Form and Checklist|Constitution|Pre-quotation disclosure, Top 20, Distribution Schedule|Audited accounts full year ended 30 June 2012|Audited accounts full year ended 30 June 2013|Audited accounts full year ended 29 June 2014|Reviewed accounts half year ended 31 December 2014|Updated pro-forma statement of financial position|Employee share option plan rules|New CEO option terms and conditions|Security Trading Policy|', '1|1|20|53|7|51|63|42|28|2|18|6|10|', 'Yes|No|No|No|No|No|No|No|No|No|No|No|No|']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-27', '0.0', '26.4', '2.19', '2.19', '2.16', '2.16', '2417254', '08:24|17:28|17:30|17:33|', 'AEST|AEST|AEST|AEST|', 'Becoming a substantial holder|Becoming a substantial holder|Becoming a substantial holder|Becoming a substantial holder|', '2|2|2|27|', 'No|No|No|No|']\n",
      "['Bundaberg Aero', 'QLD', '2015-07-28', '0.0', '23.2', '2.15', '2.16', '2.01', '2.04', '2379886', '08:26|', 'AEST|', 'Change in substantial holding|', '2|', 'No|']\n",
      "  ⋮\n",
      "['Wynyard Airport', 'TAS', '2019-09-27', '0.0', '14.4', '3.60', '3.65', '3.53', '3.53', '2260278', '', '', '', '', '']\n",
      "['Wynyard Airport', 'TAS', '2019-09-30', '0.0', '17.1', '3.53', '3.79', '3.51', '3.75', '2826881', '', '', '', '', '']\n",
      "['Wynyard Airport', 'TAS', '2019-10-01', '0.0', '19.1', '3.75', '3.77', '3.65', '3.73', '1910395', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# initialise merged_data with all 15 attributes\n",
    "merged_data = [[]]\n",
    "for attribute in weather_data_final[0]: merged_data[0].append(attribute)\n",
    "for attribute in share_price_data_final[0][1:]: merged_data[0].append(attribute)\n",
    "for attribute in announcements_data_final[0][1:]: merged_data[0].append(attribute)\n",
    "    \n",
    "for row in weather_data_final[1:]:\n",
    "    row = list(row)\n",
    "    merged_data.append(row)\n",
    "    for row_sp in share_price_data_final[1:]:\n",
    "        if row_sp[0] == row[2]: # date match\n",
    "            merged_data[-1].extend(row_sp[1:]) # attach share price entry when a match occurs with current weather entry\n",
    "    \n",
    "    merged_data[-1].extend(['','','','','']) # announcement fields\n",
    "    \n",
    "    for row_an in announcements_data_final[1:]:\n",
    "        if row_an[0] == row[2]: # date match\n",
    "            row_an = list(row_an[1:])\n",
    "            for i in range(len(row_an)):\n",
    "                merged_data[-1][10 + i] += str(row_an[i]) + '|' # separate announcements by '|' for matching date\n",
    "\n",
    "view_data(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is still a little difficult to make sense of due to the amount of attributes (15) in the merged data set. We will write the `merged_data` list to a .csv file and then it will be easier to see the final product using a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Merged_Data.csv', 'w', newline='\\n') as myfile:\n",
    "    for entry in merged_data:\n",
    "        csv.writer(myfile).writerow(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Summary Statistics\n",
    "## 3.1 Preliminary Code\n",
    "As we did before, we need to import any new modules and define any new functions that we will be using for processing our summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display # display statistics in a HTML-style table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `calculate_aggregates` function below takes in a list of lists (the data set) and a column number. It calculates the minimum, average and maximum for the attribute specified by the column number and returns these three statistics as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregates(data, column_no):\n",
    "    values = [] # define an empty list to store the column we're working with\n",
    "    for row in data:\n",
    "        values.append(float(row[column_no])) # add the element to our values list\n",
    "      \n",
    "    return [min(values), round(sum(values) / len(values), 2), max(values)] # numeric aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Share Price Data\n",
    "Recall the attributes in our `share_price_data_final` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "print(share_price_data_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the list is ordered by `Date`, let's provide a simple aggregate for the `Date` attribute by printing out the most recent date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Date\n",
      "2019-10-01\n"
     ]
    }
   ],
   "source": [
    "print('Latest Date\\n' + share_price_data_final[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other attributes in the our share price data are numerical. We will use our `calculate_aggregates` function along with the imported HTML module to produce an organised table of numerical statistics for these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:100%\"><tr> <th>Attribute</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr><tr><td>Open</td><td>1.86</td><td>4.64</td><td>9.04</td></tr><tr><td>High</td><td>1.86</td><td>4.7</td><td>9.04</td></tr><tr><td>Low</td><td>1.77</td><td>4.57</td><td>8.82</td></tr><tr><td>Close</td><td>1.83</td><td>4.63</td><td>8.98</td></tr><tr><td>Volume</td><td>0.0</td><td>1493877.29</td><td>29177016.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aggregates = []\n",
    "for column in range(1, 6):\n",
    "    aggregates.append([share_price_data_final[0][column]])\n",
    "    aggregates[-1].extend(calculate_aggregates(share_price_data_final[1:], column))\n",
    "\n",
    "header = '<tr> <th>Attribute</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr>'\n",
    "\n",
    "display(HTML(\n",
    "   '<table style=\"font-size:100%\">' + header + '<tr>{}</tr>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(value) for value in row)) for row in aggregates)\n",
    "       )\n",
    "    + '</table>'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table aggregates daily share price values over the 2015-07-24 to 2019-10-01 time period. The share price for CGC has ranged between \\\\$1.77 and $9.04, averaging around the \\\\$4.60 mark. The volume maxed out at almost 30 million trades in a single day, but averages at around 1.5 million per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Weather Data\n",
    "Recall the attributes in our `weather_data_final` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station Name', 'State', 'Date', 'Total Rainfall (mm)', 'Max Temperature (degrees Celsius)']\n"
     ]
    }
   ],
   "source": [
    "print(weather_data_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did previously, we will print out the most recent date from the `Date` attribute. For the `State` attribute, we will also calculate the number of data entries for each Australian state and territory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Date\n",
      "2019-10-01\n",
      "\n",
      "Entries per State\n",
      "ACT 0\n",
      "NSW 3186\n",
      "NT 0\n",
      "SA 2124\n",
      "QLD 4248\n",
      "TAS 4248\n",
      "VIC 1062\n",
      "WA 1062\n"
     ]
    }
   ],
   "source": [
    "print('Latest Date\\n' + weather_data_final[-1][2])\n",
    "\n",
    "states = {'ACT':0, 'NSW':0, 'NT':0, 'SA':0, 'QLD':0, 'TAS':0, 'VIC':0, 'WA':0}\n",
    "for row in weather_data_final[1:]:\n",
    "    states[row[1]] += 1\n",
    "\n",
    "print('\\nEntries per State')\n",
    "for key in states: print(key, states[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of our last three attributes, we will produce a table of statistics for `Total Rainfall (mm)` and `Max Temperature (degrees Celsius)` grouped by `Station Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:100%\"><tr><td>Daily Rainfall (mm)</td><td>Daily Max Temperature (degrees C)</td></tr><tr><td><table><tr> <th>Station</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr><tr><td>Bundaberg Aero</td><td>0.0</td><td>2.23</td><td>238.8</td></tr><tr><td>Cardwell Marine Pde</td><td>0.0</td><td>6.32</td><td>263.0</td></tr><tr><td>Coffs Harbour Airport</td><td>0.0</td><td>3.31</td><td>142.8</td></tr><tr><td>Devonport Airport</td><td>0.0</td><td>2.03</td><td>90.6</td></tr><tr><td>Gayndah Airport</td><td>0.0</td><td>1.53</td><td>88.2</td></tr><tr><td>Gingin Aero</td><td>0.0</td><td>1.83</td><td>74.0</td></tr><tr><td>Guyra Hospital</td><td>0.0</td><td>2.05</td><td>60.0</td></tr><tr><td>Launceston (Ti Tree Bend)</td><td>0.0</td><td>1.89</td><td>88.0</td></tr><tr><td>Loxton Research Centre</td><td>0.0</td><td>0.74</td><td>49.2</td></tr><tr><td>Mildura Airport</td><td>0.0</td><td>0.64</td><td>54.2</td></tr><tr><td>Renmark Aero</td><td>0.0</td><td>0.62</td><td>29.8</td></tr><tr><td>Sheffield School Farm</td><td>0.0</td><td>2.99</td><td>215.8</td></tr><tr><td>Tumbarumba Post Office</td><td>0.0</td><td>2.33</td><td>48.6</td></tr><tr><td>Walkamin Research Station</td><td>0.0</td><td>2.76</td><td>152.0</td></tr><tr><td>Wynyard Airport</td><td>0.0</td><td>2.68</td><td>127.6</td></tr><tr> <td><b>Overall</b></td> <td><b>0.0</b></td> <td><b>2.26</b></td> <td><b>263.0</b></td> </tr></table></td><td><table><tr> <th>Station</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr><tr><td>Bundaberg Aero</td><td>17.9</td><td>27.77</td><td>38.5</td></tr><tr><td>Cardwell Marine Pde</td><td>18.8</td><td>28.22</td><td>41.8</td></tr><tr><td>Coffs Harbour Airport</td><td>14.8</td><td>24.45</td><td>39.9</td></tr><tr><td>Devonport Airport</td><td>7.4</td><td>17.37</td><td>30.5</td></tr><tr><td>Gayndah Airport</td><td>16.5</td><td>29.15</td><td>40.5</td></tr><tr><td>Gingin Aero</td><td>12.1</td><td>25.19</td><td>44.4</td></tr><tr><td>Guyra Hospital</td><td>2.2</td><td>18.58</td><td>33.6</td></tr><tr><td>Launceston (Ti Tree Bend)</td><td>7.8</td><td>18.94</td><td>34.2</td></tr><tr><td>Loxton Research Centre</td><td>10.7</td><td>24.76</td><td>46.6</td></tr><tr><td>Mildura Airport</td><td>9.4</td><td>25.18</td><td>46.4</td></tr><tr><td>Renmark Aero</td><td>9.7</td><td>25.32</td><td>47.1</td></tr><tr><td>Sheffield School Farm</td><td>5.0</td><td>16.16</td><td>34.4</td></tr><tr><td>Tumbarumba Post Office</td><td>5.0</td><td>20.55</td><td>40.8</td></tr><tr><td>Walkamin Research Station</td><td>18.1</td><td>27.99</td><td>38.2</td></tr><tr><td>Wynyard Airport</td><td>8.1</td><td>17.47</td><td>33.8</td></tr><tr> <td><b>Overall</b></td> <td><b>2.2</b></td> <td><b>23.14</b></td> <td><b>47.1</b></td> </tr></table></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create dictionary where keys are stations\n",
    "groupby_stations = {}\n",
    "for row in weather_data_final[1:]:\n",
    "    if row[0] in groupby_stations:\n",
    "        groupby_stations[row[0]].append(row[1:])\n",
    "    else:\n",
    "        groupby_stations[row[0]] = [row[1:]]\n",
    "\n",
    "# for both rain and temp, calculate aggregates for each station\n",
    "rain_aggregates = []\n",
    "temp_aggregates = []\n",
    "for key in groupby_stations:\n",
    "    rain_aggregates.append([key])\n",
    "    rain_aggregates[-1].extend(calculate_aggregates(groupby_stations[key], 2))\n",
    "    temp_aggregates.append([key])\n",
    "    temp_aggregates[-1].extend(calculate_aggregates(groupby_stations[key], 3))\n",
    "\n",
    "# calculate overall aggregates for rain and temp attributes\n",
    "total_rain_aggregates = calculate_aggregates(weather_data_final[1:], 3)\n",
    "total_temp_aggregates = calculate_aggregates(weather_data_final[1:], 4)\n",
    "\n",
    "# construct a table featuring all of the aggregates\n",
    "header_rain = '<tr> <th>Station</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr>'\n",
    "trailer_rain = '<tr> <td><b>Overall</b></td> <td><b>' \n",
    "trailer_rain += str(total_rain_aggregates[0]) + '</b></td> <td><b>'\n",
    "trailer_rain += str(total_rain_aggregates[1]) + '</b></td> <td><b>'\n",
    "trailer_rain += str(total_rain_aggregates[2]) + '</b></td> </tr>'\n",
    "\n",
    "header_temp = '<tr> <th>Station</th> <th>Minimum</th> <th>Average</th> <th>Maximum</th> </tr>'\n",
    "trailer_temp = '<tr> <td><b>Overall</b></td> <td><b>' \n",
    "trailer_temp += str(total_temp_aggregates[0]) + '</b></td> <td><b>'\n",
    "trailer_temp += str(total_temp_aggregates[1]) + '</b></td> <td><b>'\n",
    "trailer_temp += str(total_temp_aggregates[2]) + '</b></td> </tr>'\n",
    "\n",
    "display(HTML(\n",
    "   '<table style=\"font-size:100%\">'\n",
    "    + '<tr><td>Daily Rainfall (mm)</td><td>Daily Max Temperature (degrees C)</td></tr><tr><td><table>'\n",
    "    + header_rain + '<tr>{}</tr>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(value) for value in row)) for row in rain_aggregates)\n",
    "       )\n",
    "     + trailer_rain + '</table></td>'\n",
    "    \n",
    "    '<td><table>' + header_temp + '<tr>{}</tr>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(value) for value in row)) for row in temp_aggregates)\n",
    "       )\n",
    "     + trailer_temp + '</table></td></tr></table>'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table contains summary statistics for daily rainfall and temperature across our 15 selected weather stations, over the 2015-07-24 to 2019-10-01 time period. Of all stations,  Cardwell Marine Pde recorded the highest maximum rainfall in a single day, 263.0mm. As for lowest maximum daily temperature in a single day, Guyra Hospital recorded the lowest out of all stations, at 2.2 degrees C.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Announcements Data\n",
    "Recall the attributes in our `announcements_data_final` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', '24-hr Time', 'Timezone', 'Title', 'Pages', 'Market Sensitive']\n"
     ]
    }
   ],
   "source": [
    "print(announcements_data_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The announcements list is ordered by date and time in reverse. Let's take a look at the time and date of the first announcement and also the most recent announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Announcement: 2015-07-24 09:54\n",
      "Latest Announcement: 2019-09-17 16:43\n"
     ]
    }
   ],
   "source": [
    "print('First Announcement:', announcements_data_final[1][0], announcements_data_final[1][1])\n",
    "print('Latest Announcement:', announcements_data_final[-1][0], announcements_data_final[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find out the percentages of *AEST* and *AEDT* values under the `Timezone` attribute and also the percentages of *Yes* and *No* values under the `Market Sensitive` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timezone \tMarket Sensitive\n",
      "AEST: 52.58% \tYes: 9.97%\n",
      "AEDT: 47.42% \tNo: 90.03%\n"
     ]
    }
   ],
   "source": [
    "AEST_total = 0; Yes_total = 0\n",
    "for row in announcements_data_final[1:]:\n",
    "    if row[2] == 'AEST': AEST_total += 1\n",
    "    if row[5] == 'Yes': Yes_total += 1\n",
    "        \n",
    "AEST_percent = round(AEST_total / len(announcements_data_final[1:]) * 100, 2)\n",
    "Yes_percent = round(Yes_total / len(announcements_data_final[1:]) * 100, 2)\n",
    "\n",
    "print('Timezone', '\\tMarket Sensitive')\n",
    "print('AEST:', str(AEST_percent) + '%', '\\tYes:', str(Yes_percent) + '%')\n",
    "print('AEDT:', str(100 - AEST_percent) + '%', '\\tNo:', str(100 - Yes_percent) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a roughly even split between AEST and AEDT timezones. This is expected since Sydney (where the ASX operates) uses AEST for half of the year and AEDT for the other half. Furthermore, only around 10% of announcements are classified as 'Market Sensitive'.\n",
    "\n",
    "For the `Title` attribute, let's find out what the 10 most common terms are amongst announcement titles. We will define *term* as any sequence of case-insensitive characters with no spaces in-between (these will mostly be words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Terms by Frequency\n",
      "substantial 118\n",
      "a 89\n",
      "holder 89\n",
      "- 79\n",
      "appendix 62\n",
      "from 51\n",
      "of 48\n",
      "to 47\n",
      "becoming 46\n",
      "change 45\n"
     ]
    }
   ],
   "source": [
    "term_distribution = {}\n",
    "for row in announcements_data_final[1:]:\n",
    "    terms = row[3].split()\n",
    "    for term in terms:\n",
    "        if term.lower() in term_distribution:\n",
    "            term_distribution[term.lower()] += 1\n",
    "        else:\n",
    "            term_distribution[term.lower()] = 0\n",
    "\n",
    "print('Top 10 Terms by Frequency')\n",
    "for key in sorted(term_distribution, key=term_distribution.get, reverse=True)[:10]: print(key, term_distribution[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, 'substantial' is most frequently occuring term in announcement titles, despite it being a fairly long and complicated word.\n",
    "\n",
    "Finally, we will produce a brief statistical summary for the numeric `Pages` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pages in Announcement Documents\n",
      "Minimum: \tAverage: \tMaximum:\n",
      "1.0\t\t12.65\t\t122.0\n"
     ]
    }
   ],
   "source": [
    "stats = calculate_aggregates(announcements_data_final[1:], 4)\n",
    "print('Number of Pages in Announcement Documents')\n",
    "#print('Minimum:', stats[0], '\\nAverage:', stats[1], '\\nMaximum:', stats[2])\n",
    "print('Minimum: \\tAverage: \\tMaximum:\\n' + str(stats[0]) + '\\t\\t' + str(stats[1]) + '\\t\\t' + str(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Announcement documents seem to average at around 12 pages. Usually, it's the annual reports that creep near the maximum of 122 pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 References\n",
    "### Data Sources\n",
    "- *Costa Group Holdings Limited (CGC.AX)*, Yahoo! Finance, https://au.finance.yahoo.com/quote/CGC.AX/history/\n",
    "- *Weather Station Directory*, Australian Government: Bureau of Meteorology, http://www.bom.gov.au/climate/data/stations/\n",
    "- *Login to CommSec, Commsec*, https://www2.commsec.com.au/Public/HomePage/Login.aspx?LoginResult=LoginRequired&r=https:%2f%2fwww2.commsec.com.au%2fquotes%2f%3fstockCode%3dCGC#/\n",
    "\n",
    "### Research\n",
    "- *Where We Grow*, Costa Group 2019, https://costagroup.com.au/Where-We-Grow\n",
    "- *Six-month financial period 2018*, Costa Group Holdings Limited, http://investors.costagroup.com.au/FormBuilder/_Resource/_module/YfnrttzbYEyUJyNrb86SEg/file/report/Six-month_financial_period_2018.pdf\n",
    "- *Growing mushrooms*, Costa Group 2019, https://costagroup.com.au/growing-mushroomsCosta \n",
    "- *Investor Centre*, Costa Group 2019, http://investors.costagroup.com.au/investor-centre/?page=asx-announcements\n",
    "- *Terms of use*, ASX, https://www.asx.com.au/about/terms-use.htm\n",
    "\n",
    "### Coding Help\n",
    "- *Grok Learning DATA1002 2019 S2*, Grok Learning, https://groklearning.com/learn/usyd-data1002-2019-s2/week1/0/\n",
    "- *How do I list all files of a directory?*, stackoverflow, https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "- *How do I output lists as a table in Jupyter Notebook?*, stackoverflow, https://stackoverflow.com/questions/35160256/how-do-i-output-lists-as-a-table-in-jupyter-notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
